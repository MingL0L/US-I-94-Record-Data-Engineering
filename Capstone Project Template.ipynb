{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Datalake I-94 Visits\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The National Travel and Tourism Office (NTTO) manages the ADIS/I-94 visitor arrivals program in cooperation with the Department of Homeland Security (DHS)/U.S. Customs and Border Protection (CBP). The I-94 provides a count of visitor arrivals to the United States (with stays of 1-night or more and visiting under certain visa types) to calculate U.S. travel and tourism volume exports.\n",
    "\n",
    "As a data engineering team, we want to build a I-94 data model for our analysis team to do statistical analysis like: Which has been the most visited city in US? For how many days? For what reasion. And how does it evolve? Is it related to other factors like temperature or population of the city?\n",
    "\n",
    "We will build an ETL to firstly stage the raw dataset in our data lake, then we will build our fact and dimension tables based on the staging tables in the data lake.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, dayofmonth, month, year, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will build an ETL to build staging tables, fact table and dimension tables in the data lake using tools like `Pandas`, `PySpark`, `AWS S3`, etc.\n",
    " And we will use three dataset: `I94 Immigration Data`, `City Temperature Data` and `U.S. City Demographic Data`.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "- **I94 immigration** [data](https://travel.trade.gov/research/reports/i94/historical/2016.html) comes from the US National Tourism and Trade Office. It is provided in . [sas7bdat](https://cran.r-project.org/web/packages/sas7bdat/vignettes/sas7bdat.pdf). The main attributes include:\n",
    "\n",
    " * `i94cit` = code for visitor origin country\n",
    " * `i94port` = code for destination USA city\n",
    " * `arrdate` = arrival date in the USA\n",
    " * `i94mode` = code for transportation mode\n",
    " * `depdate` = departure date from the USA\n",
    " * `i94visa` = code for visa type (reason for visiting)\n",
    "\n",
    "- **City Temperature** [data](https://www.kaggle.com/sudalairajkumar/daily-temperature-of-major-cities) comes from Kaggle. It is provided in csv format. The major attributes include:\n",
    "  * `Date` \n",
    "  * `AverageTemperature`\n",
    "  * `City` \n",
    "  * `State`\n",
    "  * `Country`\n",
    "  \n",
    "- **U.S. City Demographic** [data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) comes from Opendatasoft. It is provided in csv format. The major attributes include:\n",
    "  * `City` \n",
    "  * `State` \n",
    "  * `Median Age`\n",
    "  * `Male Population` \n",
    "  * `Female Population` \n",
    "  * `Total Population`\n",
    "  * `Number of Veterans` \n",
    "  * `Foreign-born` \n",
    "  * `Average Household Size`\n",
    "  * `Race` \n",
    "  * `Count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Datalake Config \n",
    "- Data input path\n",
    "- Data output path\n",
    "- AWS key and password if using AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "input_data = config.get('DATA','INPUT')\n",
    "output_data = config.get('DATA','OUTPUT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load i94 immigration dataset\n",
    "df_spark_i94 = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "                .load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark_i94.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Load us-cities-demographics\n",
    "df_spark_dem = spark.read.options(header='True',inferSchema='True',delimiter=';').csv(\"us-cities-demographics.csv\")\n",
    "df_spark_dem.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- AvgTemperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load city_temperature\n",
    "df_spark_temp = spark.read.options(header='True',inferSchema='True').csv(\"city_temperature.csv\")\n",
    "df_spark_temp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Data quality issues:\n",
    "###### i94 immigration data\n",
    "- Invalid destination cities: missing port code or ports of entry don't belong to US\n",
    "- Missing values for departure date\n",
    "\n",
    "###### US cities-demographics data\n",
    "- Invalid columns names: whitespce, '-', better to be lowercase.\n",
    "- Duplicate rows due to race counint columns which is not necessary\n",
    "\n",
    "###### Cities temperatures\n",
    "- We only want temperature info for destination cities in April 2016\n",
    "- Better to use lowercase for columns city and state for joining later with other table\n",
    "\n",
    "#### Cleaning Steps\n",
    "Drop duplicates records for all the dataframes\n",
    "\n",
    "###### i94 immigration data\n",
    "- Remove records missing invalid port of entry code\n",
    "- Remove records missing departure date\n",
    "\n",
    "###### US cities-demographics data\n",
    "- Fix invalid columns names\n",
    "- Remove columns race & count\n",
    "\n",
    "###### Cities temperatures\n",
    "- Filter records in April 2016, for US port of entry\n",
    "- Change columns city and state to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary of valid i94port codes\n",
    "re_filter = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "i94port_map = {}\n",
    "with open('mappings/i94prtl_valid.txt') as f:\n",
    "     for line in f:\n",
    "         groups = re_filter.search(line)\n",
    "         i94port_map[groups[1]]=[groups[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| 474.0|2016.0|   4.0| 103.0| 103.0|    NEW|20545.0|    2.0|   null|20547.0|  25.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1991.0|06292016|     F|  null|    VES|5.5410441233E10|91285|      WT|\n",
      "|1508.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20552.0|  16.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2000.0|06292016|     F|  null|     LX|5.5416411533E10|00016|      WT|\n",
      "|1669.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     FL|20561.0|  57.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1959.0|06292016|     M|  null|     AA|5.5457751333E10|00039|      WT|\n",
      "|2025.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20549.0|  51.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1965.0|06292016|  null|  null|     SN|5.5419979133E10|01401|      WT|\n",
      "|2048.0|2016.0|   4.0| 104.0| 104.0|    MIA|20545.0|    1.0|     FL|20554.0|   3.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2013.0|06292016|  null|  null|     UX|5.5456895133E10|00097|      WT|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean i94 immigration data\n",
    "df_spark_i94_clr = df_spark_i94.dropDuplicates()\\\n",
    "                    .na.drop(subset=[\"depdate\"])\\\n",
    "                    .filter(df_spark_i94.i94port.isin(list(i94port_map.keys())))\\\n",
    "                    .na.drop(subset=[\"matflag\"])\n",
    "df_spark_i94_clr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write to parquet partitioned by arrdate\n",
    "df_spark_i94_clr.write.partitionBy(\"arrdate\").parquet(os.path.join(output_data, \"stage_i94_immigration\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|       city|     state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|clarksville| tennessee|      29.7|          75029|            74161|          149190|             20803|        8211|                  2.64|        TN|\n",
      "|    gresham|    oregon|      36.7|          53866|            56716|          110582|              6326|       17860|                  2.67|        OR|\n",
      "|  fullerton|california|      34.5|          69549|            71300|          140849|              5394|       43404|                  2.97|        CA|\n",
      "|saint louis|  missouri|      35.0|         153026|           162659|          315685|             17060|       21802|                  2.16|        MO|\n",
      "| san marcos|california|      35.4|          45246|            47688|           92934|              5189|       21558|                  3.13|        CA|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean us-cities-demographics data\n",
    "df_spark_dem = df_spark_dem.toDF(*(c.replace(' ', '_') for c in df_spark_dem.columns))\n",
    "df_spark_dem = df_spark_dem.toDF(*(c.replace('-', '_') for c in df_spark_dem.columns))\n",
    "df_spark_dem = df_spark_dem.toDF(*[c.lower() for c in df_spark_dem.columns])\n",
    "# Remove race count columns\n",
    "df_spark_dem_clr = df_spark_dem.drop(\"race\", \"count\")\\\n",
    "                    .dropDuplicates()\\\n",
    "                    .na.drop(subset=[\"city\",\"state\"])\\\n",
    "                    .withColumn(\"city\", lower(col(\"city\")))\\\n",
    "                    .withColumn(\"state\", lower(col(\"state\")))\n",
    "df_spark_dem_clr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write us-cities-demographics to parquet \n",
    "df_spark_dem_clr.write.parquet(os.path.join(output_data, \"stage_cities_demographics\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------+----------+-----+---+----+--------------+\n",
      "|       region|country|     state|      city|month|day|year|avgtemperature|\n",
      "+-------------+-------+----------+----------+-----+---+----+--------------+\n",
      "|North America|     US|    alaska| anchorage|    4| 22|2016|          49.2|\n",
      "|North America|     US|    alaska| fairbanks|    4|  8|2016|          38.7|\n",
      "|North America|     US|   arizona|    tucson|    4| 26|2016|          64.4|\n",
      "|North America|     US|  arkansas|fort smith|    4|  7|2016|          62.2|\n",
      "|North America|     US|california|    fresno|    4| 13|2016|          62.0|\n",
      "+-------------+-------+----------+----------+-----+---+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean cities_temperatures data\n",
    "# Convert city and state name to lower case\n",
    "df_spark_temp = df_spark_temp.toDF(*[c.lower() for c in df_spark_temp.columns])\n",
    "df_spark_temp_clr = df_spark_temp.dropDuplicates()\\\n",
    "                                    .filter(df_spark_temp.country==\"US\")\\\n",
    "                                    .filter(df_spark_temp.year==2016)\\\n",
    "                                    .filter(df_spark_temp.month==4)\\\n",
    "                                    .withColumn(\"state\", lower(col(\"state\")))\\\n",
    "                                    .withColumn(\"city\", lower(col(\"city\")))\n",
    "df_spark_temp_clr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write cities temperatures data to parquet\n",
    "df_spark_temp_clr.write.parquet(os.path.join(output_data, \"stage_uscities_temperatures\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "We will use star schema as below:\n",
    "![Tux, the Linux mascot](schema_i94.png)\n",
    "\n",
    "###### 3.1.1 Fact table\n",
    "Our fact table **fact_i94visits** will contain informations from the i94 immigration data \n",
    "joined with daily average temperature on the port city and arrival date.\n",
    "\n",
    "###### 3.1.2 Dimention tables\n",
    "- `dim_us_ports` contains informations like US port of entry code, city, state code and state name.\n",
    "- `dim_visa` maps visa type which gives information like reason for visiting.\n",
    "- `dim_countries` tells which country does visitor come from.\n",
    "- `dim_travelmode` gives mode of transportation: air, land or sea.\n",
    "- `dim_demographics`contains median age and population informations about US port city.   \n",
    "- `dim_date` contains date information like year, month, day, week of year and weekday.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are described below:\n",
    "1. Load raw dataset from source into Spark dataframe: `df_spark_i94`,  `df_spark_dem` and `df_spark_temp` for one month.\n",
    "2. Clean each Spark dataframe as decscibed in *Step 2 Cleaning steps* and write each cleaned dataframe into parquet as staging table: `stage_i94_immigration`, `stage_cities_demographics` and `stage_uscities_temperatures`.\n",
    "3. Create and load dimension tables: `dim_us_ports`, `dim_visa`, `dim_countries`, `dim_travelmode` and `dim_demographics`.\n",
    "4. Create and load fact table `fact_i94_visits` joining `stage_i94_immigration` and `stage_uscities_temperatures`.\n",
    "5. Create and load dimension tables  and `dim_date`.\n",
    "6. Quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from etl import stage_i94, \\\n",
    "                stage_demographics, \\\n",
    "                stage_temperatures, \\\n",
    "                load_dim_port, \\\n",
    "                load_dim_country, \\\n",
    "                load_dim_visa, \\\n",
    "                load_dim_travelmode, \\\n",
    "                load_dim_demographics, \\\n",
    "                build_fact_i94visits, \\\n",
    "                load_dim_date, \\\n",
    "                check_count, \\\n",
    "                check_uniquekey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.1 Create staging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_i94 start\n",
      "stage_i94 end\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| 474.0|2016.0|   4.0| 103.0| 103.0|    NEW|20545.0|    2.0|   null|20547.0|  25.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1991.0|06292016|     F|  null|    VES|5.5410441233E10|91285|      WT|\n",
      "|1508.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20552.0|  16.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2000.0|06292016|     F|  null|     LX|5.5416411533E10|00016|      WT|\n",
      "|1669.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     FL|20561.0|  57.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1959.0|06292016|     M|  null|     AA|5.5457751333E10|00039|      WT|\n",
      "|2025.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20549.0|  51.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1965.0|06292016|  null|  null|     SN|5.5419979133E10|01401|      WT|\n",
      "|2048.0|2016.0|   4.0| 104.0| 104.0|    MIA|20545.0|    1.0|     FL|20554.0|   3.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2013.0|06292016|  null|  null|     UX|5.5456895133E10|00097|      WT|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load, Clean and Stage I94 immigration dataset\n",
    "stage_i94(spark, \\\n",
    "          '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', \\\n",
    "          output_data, 'stage_i94_immigration').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_demographics start\n",
      "stage_demographics end\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|       city|     state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|clarksville| tennessee|      29.7|          75029|            74161|          149190|             20803|        8211|                  2.64|        TN|\n",
      "|    gresham|    oregon|      36.7|          53866|            56716|          110582|              6326|       17860|                  2.67|        OR|\n",
      "|  fullerton|california|      34.5|          69549|            71300|          140849|              5394|       43404|                  2.97|        CA|\n",
      "|saint louis|  missouri|      35.0|         153026|           162659|          315685|             17060|       21802|                  2.16|        MO|\n",
      "| san marcos|california|      35.4|          45246|            47688|           92934|              5189|       21558|                  3.13|        CA|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load, Clean and Stage demographics dataset\n",
    "stage_demographics(spark, \\\n",
    "          \"us-cities-demographics.csv\", \\\n",
    "          output_data, 'stage_cities_demographics').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_temperatures start\n",
      "stage_temperatures end\n",
      "+-------------+-------+----------+----------+-----+---+----+--------------+\n",
      "|       region|country|     state|      city|month|day|year|avgtemperature|\n",
      "+-------------+-------+----------+----------+-----+---+----+--------------+\n",
      "|North America|     US|    alaska| anchorage|    4| 22|2016|          49.2|\n",
      "|North America|     US|    alaska| fairbanks|    4|  8|2016|          38.7|\n",
      "|North America|     US|   arizona|    tucson|    4| 26|2016|          64.4|\n",
      "|North America|     US|  arkansas|fort smith|    4|  7|2016|          62.2|\n",
      "|North America|     US|california|    fresno|    4| 13|2016|          62.0|\n",
      "+-------------+-------+----------+----------+-----+---+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load, Clean and Stage temperatures dataset\n",
    "stage_temperatures(spark, \\\n",
    "          \"city_temperature.csv\", \\\n",
    "          output_data, 'stage_uscities_temperatures').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2 Build dimension tables from mapping text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dim_port start\n",
      "load_dim_port end\n",
      "+----------+---------+---------+-------+\n",
      "|state_code|port_code|     city|  state|\n",
      "+----------+---------+---------+-------+\n",
      "|        AZ|      DOU|  douglas|arizona|\n",
      "|        AZ|      LUK|lukeville|arizona|\n",
      "|        AZ|      NAC|     naco|arizona|\n",
      "|        AZ|      NOG|  nogales|arizona|\n",
      "|        AZ|      PHO|  phoenix|arizona|\n",
      "+----------+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and load dimension : dim_us_ports\n",
    "load_dim_port(spark, output_data, 'dim_us_ports').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dim_country start\n",
      "load_dim_country end\n",
      "+------------+--------------------+\n",
      "|country_code|             country|\n",
      "+------------+--------------------+\n",
      "|         582|MEXICO Air Sea, a...|\n",
      "|         236|         AFGHANISTAN|\n",
      "|         101|             ALBANIA|\n",
      "|         316|             ALGERIA|\n",
      "|         102|             ANDORRA|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and load dimension : dim_countries\n",
    "load_dim_country(spark, output_data, 'dim_countries').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dim_visa start\n",
      "load_dim_visa end\n",
      "+---------+--------+\n",
      "|visa_code|    visa|\n",
      "+---------+--------+\n",
      "|        1|Business|\n",
      "|        2|Pleasure|\n",
      "|        3| Student|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and load dimension : dim_visa\n",
    "load_dim_visa(spark, output_data, 'dim_us_visa').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dim_travelmode start\n",
      "load_dim_travelmode end\n",
      "+---------+------------+\n",
      "|mode_code|        mode|\n",
      "+---------+------------+\n",
      "|        1|         Air|\n",
      "|        2|         Sea|\n",
      "|        3|        Land|\n",
      "|        9|Not reported|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and load dimension : dim_travelmode\n",
    "load_dim_travelmode(spark, output_data, 'dim_travelmode').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dim_demographics start\n",
      "load_dim_demographics end\n",
      "+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------+\n",
      "|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|port_code|\n",
      "+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------+\n",
      "|      33.8|         223960|           239915|          463875|             18572|       32016|                  2.15|        GA|      ATL|\n",
      "|      31.4|         100135|           109673|          209808|              7288|       17735|                  2.36|        NY|      ROC|\n",
      "|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|      FMY|\n",
      "|      34.1|         312237|           343523|          655760|             31189|       43318|                  2.55|        TN|      MEM|\n",
      "|      34.1|         309227|           322036|          631263|             41843|       80165|                  2.58|        OK|      OKC|\n",
      "+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and load dimension : dim_demographics\n",
    "load_dim_demographics(spark, output_data, 'dim_demographics').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### 4.1.3 Build fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_fact_i94visits start\n",
      "build_fact_i94visits end\n",
      "+---------+-----------+-----------+----+-------+------+-------+-------+------+--------------+\n",
      "|    cicid|arrdate_iso|depdate_iso|stay|i94port|i94cit|i94mode|i94visa|i94bir|avgtemperature|\n",
      "+---------+-----------+-----------+----+-------+------+-------+-------+------+--------------+\n",
      "|5659207.0| 2016-04-30| 2016-05-08|   8|    NEW| 103.0|    1.0|    2.0|  55.0|          null|\n",
      "|5659318.0| 2016-04-30| 2016-05-21|  21|    HOU| 103.0|    1.0|    2.0|  51.0|          76.7|\n",
      "|5659356.0| 2016-04-30| 2016-05-04|   4|    NYC| 103.0|    1.0|    2.0|  29.0|          null|\n",
      "|5659405.0| 2016-04-30| 2016-05-07|   7|    NYC| 103.0|    1.0|    2.0|  55.0|          null|\n",
      "|5659433.0| 2016-04-30| 2016-05-08|   8|    NYC| 103.0|    1.0|    2.0|  46.0|          null|\n",
      "+---------+-----------+-----------+----+-------+------+-------+-------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build fact table : dim_demographics\n",
    "build_fact_i94visits(spark, output_data, 'fact_i94visits').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### 4.1.4 Build dimension table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dim_date start\n",
      "load_dim_date end\n",
      "+-----------+----+-----+---+----+-------+\n",
      "|arrdate_iso|year|month|day|week|weekday|\n",
      "+-----------+----+-----+---+----+-------+\n",
      "| 2016-04-25|2016|    4| 25|  17|    Mon|\n",
      "| 2016-04-22|2016|    4| 22|  16|    Fri|\n",
      "| 2016-04-30|2016|    4| 30|  17|    Sat|\n",
      "| 2016-04-26|2016|    4| 26|  17|    Tue|\n",
      "| 2016-04-04|2016|    4|  4|  14|    Mon|\n",
      "+-----------+----+-----+---+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dimension date_table\n",
    "load_dim_date(spark, output_data, 'dim_date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "For data quality, we will checks records count and unique key of our final fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check succes for i94 visits with 2746459 records.\n"
     ]
    }
   ],
   "source": [
    "# Data quality check: Count records\n",
    "check_count(spark, output_data, 'fact_i94visits', \"i94 visits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique key check succes for i94 visits.\n"
     ]
    }
   ],
   "source": [
    "# Data quality check: Unique key\n",
    "check_uniquekey(spark, output_data, 'fact_i94visits', ['cicid'], \"i94 visits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stop spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### fact_i94visits\n",
    "Fact table extracted from the I94 immigration data（from US National Tourism and Trade Office）joind with city temperature dataset（from kaggle)\n",
    "* `cicid` = key id\n",
    "* `arrdate_iso` = arrival date\n",
    "* `depdate_iso` = depature date\n",
    "* `stay` = number of days\n",
    "* `i94port` = 3 character code of destination city\n",
    "* `i94cit` = 3 digit code of origin country\n",
    "* `i94mode` = 1 digit transportation mode code\n",
    "* `i94visa` = 1 digit visa type code\n",
    "* `i94bir` = Age of traveler (7 age groupings, mean and median)\n",
    "* `avgtemperature` = average temperature in fahrenheit\n",
    "\n",
    "#### dim_countries\n",
    "Dimension table extracted from mapping text\n",
    "* `country_code` = 3 digit code of origin country\n",
    "* `country` = origin country name\n",
    "\n",
    "#### dim_date\n",
    "Dimension table extracted from fact_i94visits\n",
    "* `arrdate_iso` = arrival date\n",
    "* `year` = arrival year\n",
    "* `month` = arrival month\n",
    "* `day` = arrival day of month\n",
    "* `week` = arrival week of year\n",
    "* `weekday` = arrival weekday\n",
    "\n",
    "#### dim_demographics\n",
    "Dimension table extracted from U.S. City Demographic Data (OpenSoft)\n",
    "* `port_code` = US port city code\n",
    "* `state_code` = US state code\n",
    "* `median_age` = median age\n",
    "* `male_population` = male population\n",
    "* `female_population` = female population\n",
    "* `total_population` = total population\n",
    "* `number_of_veterans` = number of veterans\n",
    "* `foreign_born` = number of foreign born\n",
    "* `average_household_size` = average household size\n",
    "\n",
    "#### dim_travelmode\n",
    "Dimension table extracted from mapping text\n",
    "* `mode_code` = 1 digit code of transportation mode\n",
    "* `mode` = Mode of transportation (air, land, sea)\n",
    "\n",
    "#### dim_us_ports\n",
    "Dimension table extracted from mapping text\n",
    "* `state_code` = 1 digit code of transportation mode\n",
    "* `port_code` = 1 digit code of transportation mode\n",
    "* `city` = 1 digit code of transportation mode\n",
    "* `state` = 1 digit code of transportation mode\n",
    "\n",
    "#### dim_us_visa\n",
    "Dimension table extracted from mapping text\n",
    "* `visa_code` = 1 digit code of visa type\n",
    "* `visa` = Type of visa (business, pleasure, student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "  - In this project, we have used pandas, Apache Spark and AWS S3 to build ETL pipline, data model and datalake. Pandas is good for dealing small dataset like the mapping text files. And once we start to process large volume dataset like i94 immigration data which contains over 3 million rows per month, we use Apache Spark, cause it executes much faster by caching data in memory across multiple parallel operations. Finally, to implement our datalake, we use AWS S3 to store our staging tables, fact and dimension tables. \n",
    "  \n",
    "\n",
    "* How often the data should be updated and why.\n",
    "\n",
    "    - It depends on how often the raw data be updated from the source. In our case, the I94 immigration data is updated monthly from the source, so it's better to run the ETL monthly. \n",
    "    \n",
    "\n",
    "* How to approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    - If the data was increased by 100 times, we may need a Spark cluster setup on AWS EMR or other cluster computing provider.\n",
    "\n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - We may use a scheduler tool for data pipline to manage the regular jobs, airflow will be a good solution.\n",
    "\n",
    "\n",
    " * The database needed to be accessed by 100+ people.\n",
    "    - We may need a powerful resource manager which allows us to share multiple applications through a common resource.\n",
    "      We could use Apache Yarn to increase the system efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
